{
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8008863,
          "sourceType": "datasetVersion",
          "datasetId": 4717182
        },
        {
          "sourceId": 8085758,
          "sourceType": "datasetVersion",
          "datasetId": 4773033
        },
        {
          "sourceId": 8086168,
          "sourceType": "datasetVersion",
          "datasetId": 4773314
        },
        {
          "sourceId": 8094382,
          "sourceType": "datasetVersion",
          "datasetId": 4779050
        },
        {
          "sourceId": 8095026,
          "sourceType": "datasetVersion",
          "datasetId": 4779519
        },
        {
          "sourceId": 11261,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 8332,
          "modelId": 3301
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4190.684325,
      "end_time": "2023-12-17T22:21:21.061832",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-12-17T21:11:30.377507",
      "version": "2.4.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117"
      ],
      "metadata": {
        "papermill": {
          "duration": 12.837112,
          "end_time": "2023-12-17T21:11:46.656292",
          "exception": false,
          "start_time": "2023-12-17T21:11:33.81918",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-05-04T09:42:35.346415Z",
          "iopub.execute_input": "2024-05-04T09:42:35.347503Z",
          "iopub.status.idle": "2024-05-04T09:42:48.803657Z",
          "shell.execute_reply.started": "2024-05-04T09:42:35.347462Z",
          "shell.execute_reply": "2024-05-04T09:42:48.802635Z"
        },
        "trusted": true,
        "id": "z_J-g9OE0-ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers==\"4.38.2\"\n",
        "!pip install -q accelerate\n",
        "!pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install -q -U datasets"
      ],
      "metadata": {
        "papermill": {
          "duration": 25.835818,
          "end_time": "2023-12-17T21:12:12.500193",
          "exception": false,
          "start_time": "2023-12-17T21:11:46.664375",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-05-04T09:42:48.806142Z",
          "iopub.execute_input": "2024-05-04T09:42:48.807082Z",
          "iopub.status.idle": "2024-05-04T09:43:39.791572Z",
          "shell.execute_reply.started": "2024-05-04T09:42:48.807039Z",
          "shell.execute_reply": "2024-05-04T09:43:39.790252Z"
        },
        "trusted": true,
        "id": "VzTksGAZ0-ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/trl\n",
        "!pip install -q -U git+https://github.com/huggingface/peft"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:43:39.793182Z",
          "iopub.execute_input": "2024-05-04T09:43:39.793522Z",
          "iopub.status.idle": "2024-05-04T09:44:35.946958Z",
          "shell.execute_reply.started": "2024-05-04T09:43:39.793492Z",
          "shell.execute_reply": "2024-05-04T09:44:35.945860Z"
        },
        "trusted": true,
        "id": "JqPQHPA30-ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "papermill": {
          "duration": 0.015679,
          "end_time": "2023-12-17T21:12:12.539544",
          "exception": false,
          "start_time": "2023-12-17T21:12:12.523865",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-05-04T09:44:35.949375Z",
          "iopub.execute_input": "2024-05-04T09:44:35.949704Z",
          "iopub.status.idle": "2024-05-04T09:44:35.955016Z",
          "shell.execute_reply.started": "2024-05-04T09:44:35.949675Z",
          "shell.execute_reply": "2024-05-04T09:44:35.953946Z"
        },
        "trusted": true,
        "id": "hKBrbA110-ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01423,
          "end_time": "2023-12-17T21:12:12.576944",
          "exception": false,
          "start_time": "2023-12-17T21:12:12.562714",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-05-04T09:44:35.956820Z",
          "iopub.execute_input": "2024-05-04T09:44:35.957168Z",
          "iopub.status.idle": "2024-05-04T09:44:35.972834Z",
          "shell.execute_reply.started": "2024-05-04T09:44:35.957142Z",
          "shell.execute_reply": "2024-05-04T09:44:35.971942Z"
        },
        "trusted": true,
        "id": "4d0SEGvN0-ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U datasets==2.17.0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:44:35.974062Z",
          "iopub.execute_input": "2024-05-04T09:44:35.974365Z",
          "iopub.status.idle": "2024-05-04T09:44:49.371465Z",
          "shell.execute_reply.started": "2024-05-04T09:44:35.974338Z",
          "shell.execute_reply": "2024-05-04T09:44:49.370291Z"
        },
        "trusted": true,
        "id": "eotv6IvX0-ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import transformers\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainingArguments,\n",
        "                          pipeline,\n",
        "                          logging)\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig\n",
        "import bitsandbytes as bnb\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "papermill": {
          "duration": 19.450408,
          "end_time": "2023-12-17T21:12:32.05101",
          "exception": false,
          "start_time": "2023-12-17T21:12:12.600602",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-05-04T09:44:49.372914Z",
          "iopub.execute_input": "2024-05-04T09:44:49.373225Z",
          "iopub.status.idle": "2024-05-04T09:44:58.600366Z",
          "shell.execute_reply.started": "2024-05-04T09:44:49.373196Z",
          "shell.execute_reply": "2024-05-04T09:44:58.599267Z"
        },
        "trusted": true,
        "id": "98IPpHM30-ZJ",
        "outputId": "e5422bf5-8060-4441-fa53-fc56fd6dc47a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-05-04 09:44:53.876311: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-04 09:44:53.876388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-04 09:44:53.878001: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"/kaggle/input/gemma/transformers/7b-it/1\"\n",
        "\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "EOS_TOKEN = tokenizer.eos_token"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:44:58.601668Z",
          "iopub.execute_input": "2024-05-04T09:44:58.602497Z",
          "iopub.status.idle": "2024-05-04T09:45:15.269392Z",
          "shell.execute_reply.started": "2024-05-04T09:44:58.602458Z",
          "shell.execute_reply": "2024-05-04T09:45:15.268388Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "6fc8c600ebec407eaf458de79fc9b8a1"
          ]
        },
        "id": "0mXT4g3v0-ZL",
        "outputId": "cb16a02c-aaca-43d0-a678-258f1c86568a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fc8c600ebec407eaf458de79fc9b8a1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:15.270942Z",
          "iopub.execute_input": "2024-05-04T09:45:15.271869Z",
          "iopub.status.idle": "2024-05-04T09:45:15.280725Z",
          "shell.execute_reply.started": "2024-05-04T09:45:15.271830Z",
          "shell.execute_reply": "2024-05-04T09:45:15.279556Z"
        },
        "trusted": true,
        "id": "-8oiM_Qj0-ZM",
        "outputId": "281319ba-27e2-4d1f-f5e7-9a5cd5a76a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "GemmaConfig {\n  \"_name_or_path\": \"/kaggle/input/gemma/transformers/7b-it/1\",\n  \"architectures\": [\n    \"GemmaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"head_dim\": 256,\n  \"hidden_act\": \"gelu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 24576,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"gemma\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 16,\n  \"pad_token_id\": 0,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"float16\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": false,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.38.2\",\n  \"use_cache\": false,\n  \"vocab_size\": 256000\n}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T09:45:15.283829Z",
          "iopub.execute_input": "2024-05-04T09:45:15.284130Z",
          "iopub.status.idle": "2024-05-04T09:45:15.295453Z",
          "shell.execute_reply.started": "2024-05-04T09:45:15.284104Z",
          "shell.execute_reply": "2024-05-04T09:45:15.294453Z"
        },
        "trusted": true,
        "id": "CllhLk1q0-ZN",
        "outputId": "61742abd-f923-4da1-d987-ee3c3744fad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n    (layers): ModuleList(\n      (0-27): 28 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loading & Basic Preprocessing"
      ],
      "metadata": {
        "id": "zELVCu260-ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_1 = \"/kaggle/input/finance-bench-dataset/10_labels.csv\"\n",
        "file_path_2 = \"/kaggle/input/finance-bench-dataset/syntheses_10.csv\"\n",
        "Model_Generated_Data = pd.read_csv(file_path_1)\n",
        "Finance_data = pd.read_csv(file_path_2)\n",
        "print(Finance_data.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "FmDbClN90-ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce Finance_data to the first 50 rows\n",
        "Finance_Reduced_Test = Finance_data.head(50)\n",
        "Finance_Reduced_Train = Finance_data.tail(100)\n",
        "# Print the reduced DataFrame to verify\n",
        "print(len(Finance_Reduced_Train))\n",
        "print(len(Finance_Reduced_Test))"
      ],
      "metadata": {
        "trusted": true,
        "id": "t5nc0yd10-ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero Shot Prompt Engineering"
      ],
      "metadata": {
        "id": "HOSARNz-0-ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a financial chatbot trained to answer questions based on the information provided.\n",
        "Your responses should be directly sourced from the content of these evidence_text(context).\n",
        "When asked a question, ensure that your answer is explicitly supported by the text and do not\n",
        "include any external information, interpretations, or assumptions not clearly stated in the evidence_text(context).\n",
        "\n",
        "Your primary focus should be on accuracy, specificity, and adherence to the information in the evidence_text(context),\n",
        "particularly regarding financial statements, company performance, and market positions.\"\"\"\n",
        "\n",
        "count = 0\n",
        "syntheses_with_gemma = []\n",
        "\n",
        "for idx, row in Finance_Reduced_Test.iterrows():\n",
        "    question = row['question']\n",
        "    evidence_text = row['evidence_text']\n",
        "\n",
        "    prompt = f\"\"\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\n",
        "        {question}\\n{evidence_text}\\n<end_of_turn>\\n\"\"\"\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate response\n",
        "    output = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=100)\n",
        "\n",
        "    # Decode the response\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    #print(generated_text)\n",
        "\n",
        "    #Finding Extracted Answer\n",
        "    index_of_answer = generated_text.find(\"<end_of_turn>\")\n",
        "\n",
        "    # Extract the text after \"Answer:\"\n",
        "    answer_text = generated_text[index_of_answer + len(\"<end_of_turn>\"):].strip()\n",
        "    syntheses_with_gemma.append(answer_text)\n",
        "    #print(answer_text)\n",
        "\n",
        "    #Comparing the answer with the base answer\n",
        "#     dash_line = '-'.join('' for x in range(100))\n",
        "#     Answer = Finance_Reduced_Test.loc[idx, 'answer']\n",
        "#     print(f'BASELINE HUMAN ANSWER:\\n{Answer}\\n')\n",
        "#     print(dash_line)\n",
        "#     print(f'MODEL GENERATION - ZERO SHOT:\\n{answer_text}')\n",
        "#     print(dash_line)\n",
        "    print(count+1)\n",
        "    count = count + 1\n",
        "\n",
        "#Appending it to main file\n",
        "Finance_Reduced_Test['Generated_BY_GEMMA'] = syntheses_with_gemma"
      ],
      "metadata": {
        "trusted": true,
        "id": "Jd7n-LRu0-ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Finance_Reduced_Test.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "nUPWgdt80-ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "Zero_Shot_Gemma = Finance_Reduced_Test.rename(columns={'question': 'question', 'answer': 'answer', 'evidence_text': 'evidence_text', 'syntheses': 'syntheses', 'Generated_BY_GEMMA': 'Generated_BY_GEMMA'})\n",
        "Zero_Shot_Gemma.to_csv('Zero_Shot_Gemma.csv', index=False)\n",
        "FileLink('Zero_Shot_Gemma.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "WlG3otK_0-ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Zero_Shot_Gemma.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Y4EpTLGQ0-ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero Shot Test"
      ],
      "metadata": {
        "id": "9Z5l5MSw0-ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_3 = \"/kaggle/input/zero-shot-gemma/Zero_Shot_Gemma.csv\"\n",
        "zero_shot_test = pd.read_csv(file_path_3)\n",
        "zero_shot_test.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Qm47YOGR0-ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Selecting the 'answer' and 'Generated_BY_GEMMA' columns\n",
        "# selected_columns = ['answer', 'Generated_BY_GEMMA']\n",
        "\n",
        "# # Printing the first 50 rows with a dash line between the columns\n",
        "# for index, row in zero_shot_test[selected_columns].head(50).iterrows():\n",
        "#     print(f\"{row['answer']} --------- {row['Generated_BY_GEMMA']}\")\n",
        "#     print('--' * 50)  # Dash line separator"
      ],
      "metadata": {
        "trusted": true,
        "id": "W5qZxy1K0-ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from typing import List\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "trusted": true,
        "id": "NuGis3kG0-ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rouge_l(candidate, reference):\n",
        "    m, n = len(candidate), len(reference)\n",
        "    #print(m,n)\n",
        "    dp_table = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if candidate[i - 1] == reference[j - 1]:\n",
        "                dp_table[i][j] = dp_table[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                dp_table[i][j] = max(dp_table[i - 1][j], dp_table[i][j - 1])\n",
        "\n",
        "    return dp_table[m][n] / n"
      ],
      "metadata": {
        "trusted": true,
        "id": "84V-_qdR0-ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "trusted": true,
        "id": "WB5eIA4n0-ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "trusted": true,
        "id": "zZDyEwmp0-ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity_score(answer,syntheses):\n",
        "    tokens_answer = preprocess_text(answer)\n",
        "    tokens_syntheses = preprocess_text(syntheses)\n",
        "    str_answer = ' '.join(tokens_answer)\n",
        "    str_syntheses = ' '.join(tokens_syntheses)\n",
        "    freqdist_answer = nltk.FreqDist(str_answer.split())\n",
        "    freqdist_syntheses = nltk.FreqDist(str_syntheses.split())\n",
        "    # Extract frequencies for unique tokens in both texts\n",
        "    unique_tokens = set(freqdist_answer.keys()).union(freqdist_syntheses.keys())\n",
        "\n",
        "    freq_answer = [freqdist_answer[token] for token in unique_tokens]\n",
        "    freq_syntheses = [freqdist_syntheses[token] for token in unique_tokens]\n",
        "\n",
        "    vector_answer = np.array(freq_answer).reshape(1, -1)\n",
        "    vector_syntheses = np.array(freq_syntheses).reshape(1, -1)\n",
        "\n",
        "    similarity_score = cosine_similarity(vector_answer, vector_syntheses)[0][0]\n",
        "\n",
        "    return similarity_score"
      ],
      "metadata": {
        "trusted": true,
        "id": "UsKrv3j50-ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer\n",
        "\n",
        "total_average_rouge_l_scores_1 = []\n",
        "total_average_cosine_similarity_scores_1  = []\n",
        "\n",
        "num_labels_1 = []\n",
        "count = 0\n",
        "\n",
        "for i in range(5):\n",
        "    count += 10\n",
        "    num_labels_1.append(count)\n",
        "    #print(num_labels)\n",
        "    df = zero_shot_test.head(count)\n",
        "    #print(len(df))\n",
        "\n",
        "    rouge_l_scores = []\n",
        "    cosine_similarity_scores = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        answer = row['answer']\n",
        "        syntheses = row['Generated_BY_GEMMA']\n",
        "\n",
        "        compute_sim_score = compute_similarity_score(answer,syntheses)\n",
        "        cosine_similarity_scores.append(compute_sim_score)\n",
        "\n",
        "        rouge_l_score = compute_rouge_l(answer, syntheses)\n",
        "        rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "\n",
        "    total_average_cosine_similarity_score = sum(cosine_similarity_scores) / len(cosine_similarity_scores)\n",
        "    total_average_cosine_similarity_scores_1.append(total_average_cosine_similarity_score)\n",
        "\n",
        "\n",
        "    total_average_rouge_l_score = sum(rouge_l_scores)/len(rouge_l_scores)\n",
        "    total_average_rouge_l_scores_1.append(total_average_rouge_l_score)\n",
        "\n",
        "    #print(total_average_cosine_similarity_scores_1)\n",
        "    print(total_average_rouge_l_scores_1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "vmKeU56Q0-ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(num_labels_1, total_average_rouge_l_scores_1, marker='o', linestyle='-', color='skyblue', linewidth=2)\n",
        "\n",
        "plt.xlabel('Number of Labels')\n",
        "plt.ylabel('Total Average ROUGE-L Score for 5 Epoch')\n",
        "plt.title(' After Fine-Tuning ROUGE-L Score Comparison for 5 Epoch')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(num_labels_1, total_average_cosine_similarity_scores_1, marker='o', linestyle='-', color='skyblue', linewidth=2)\n",
        "plt.xlabel('Number of Labels')\n",
        "plt.ylabel('Total Average Cosine Similarity Score for 5 Epoch')\n",
        "plt.title(' After Fine-Tuning Cosine Similarity Score for 5 Epoch')"
      ],
      "metadata": {
        "trusted": true,
        "id": "k86UK3x00-ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero Shot Finished"
      ],
      "metadata": {
        "id": "YHOceDcX0-ZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot Prompt Engineering"
      ],
      "metadata": {
        "id": "Ij0MXj8F0-ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Few_Shot_7 = Finance_Reduced_Test[45:50]\n",
        "len(Few_Shot_7)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rpCvEzSu0-ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a financial chatbot trained to answer questions based on the information provided.\n",
        "Your responses should be directly sourced from the content of these evidence_text(context).\n",
        "When asked a question, ensure that your answer is explicitly supported by the text and do not\n",
        "include any external information, interpretations, or assumptions not clearly stated in the evidence_text(context).\n",
        "\n",
        "Your primary focus should be on accuracy, specificity, and adherence to the information in the evidence_text(context),\n",
        "particularly regarding financial statements, company performance, and market positions.\n",
        "\n",
        "Here are some sample Questions, evidence text and answers.\n",
        "\n",
        "question 1: Was there any drop in Cash & Cash equivalents between FY 2023 and Q2 of FY2024?\n",
        "evidence_text 1: July 29, 2023 January 28, 2023 July 30, 2022 Cash and cash equivalents $ 1,093 $ 1,874 $ 840\n",
        "answer 1: Yes, there was a decline of ~42% between FY2023 and Q2 of FY 2024.\n",
        "\n",
        "question 2: We need to calculate a financial metric by using information only provided within the balance sheet. Please answer the following question: what is Boeing's year end FY2018 net property, plant, and equipment (in USD millions)?\n",
        "evidence_text 2: Table of Contents The Boeing Company and Subsidiaries Consolidated Statements of Financial Position   (Dollars in millions, except per share data)       December 31, 2018   2017 Assets       Cash and cash equivalents $7,637   $8,813 Short-term and other investments 927   1,179 Accounts receivable, net 3,879   2,894 Unbilled receivables, net 10,025   8,194 Current portion of customer financing, net 460   309 Inventories 62,567   61,388 Other current assets 2,335   2,417 Total current assets 87,830   85,194 Customer financing, net 2,418   2,756 Property, plant and equipment, net 12,645   12,672 Goodwill 7,840   5,559 Acquired intangible assets, net 3,429   2,573 Deferred income taxes 284   321 Investments 1,087   1,260 Other assets, net of accumulated amortization of $503 and $482 1,826   2,027 Total assets $117,359   $112,362 Liabilities and equity       Accounts payable $12,916   $12,202 Accrued liabilities 14,808   13,069 Advances and progress billings 50,676   48,042 Short-term debt and current portion of long-term debt 3,190   1,335 Total current liabilities 81,590   74,648 Deferred income taxes 1,736   2,188 Accrued retiree health care 4,584   5,545 Accrued pension plan liability, net 15,323   16,471 Other long-term liabilities 3,059   2,015 Long-term debt 10,657   9,782 Shareholders’ equity:       Common stock, par value $5.00 – 1,200,000,000 shares authorized; 1,012,261,159 shares issued 5,061   5,061 Additional paid-in capital 6,768   6,804 Treasury stock, at cost (52,348)   (43,454) Retained earnings 55,941   49,618 Accumulated other comprehensive loss (15,083)   (16,373) Total shareholders’ equity 339   1,656 Noncontrolling interests 71   57 Total equity 410   1,713 Total liabilities and equity $117,359   $112,362 See Notes to the Consolidated Financial Statements on pages 54 – 113 . 50\n",
        "answer 2: $12645.00\n",
        "\n",
        "question 3: At the Pepsico AGM held on May 3, 2023, what was the outcome of the shareholder vote on the shareholder proposal for a congruency report by Pepsico on net-zero emissions policies?\n",
        "evidence_text 3: (8) The shareholder proposal regarding a congruency report on net-zero emissions policies was defeated: For 19,718,780 Against 977,228,788\n",
        "answer 3: The shareholder proposal for a congruency report by Pepsico on net-zero emissions policies was defeated.\n",
        "\n",
        "Now give answer to questions provided below from the evidence text.\"\"\"\n",
        "\n",
        "count = 0\n",
        "syntheses_with_gemma = []\n",
        "\n",
        "for idx, row in Few_Shot_7.iterrows():\n",
        "    question = row['question']\n",
        "    evidence_text = row['evidence_text']\n",
        "\n",
        "    prompt = f\"\"\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\n",
        "        {question}\\n{evidence_text}\\n<end_of_turn>\\n\"\"\"\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate response\n",
        "    output = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=100)\n",
        "\n",
        "    # Decode the response\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    #print(generated_text)\n",
        "\n",
        "    #Finding Extracted Answer\n",
        "    index_of_answer = generated_text.find(\"<end_of_turn>\")\n",
        "\n",
        "    # Extract the text after \"Answer:\"\n",
        "    answer_text = generated_text[index_of_answer + len(\"<end_of_turn>\"):].strip()\n",
        "    syntheses_with_gemma.append(answer_text)\n",
        "    #print(answer_text)\n",
        "\n",
        "    #Comparing the answer with the base answer\n",
        "#     dash_line = '-'.join('' for x in range(100))\n",
        "#     Answer = Finance_Reduced_Test.loc[idx, 'answer']\n",
        "#     print(f'BASELINE HUMAN ANSWER:\\n{Answer}\\n')\n",
        "#     print(dash_line)\n",
        "#     print(f'MODEL GENERATION - ZERO SHOT:\\n{answer_text}')\n",
        "#     print(dash_line)\n",
        "    print(count+1)\n",
        "    count = count + 1\n",
        "\n",
        "#Appending it to main file\n",
        "Few_Shot_7['Generated_BY_GEMMA'] = syntheses_with_gemma"
      ],
      "metadata": {
        "trusted": true,
        "id": "IXLJI9tW0-ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Few_Shot_7.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "iB_rTfam0-ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "Few_Shot_Gemma_7 = Few_Shot_7.rename(columns={'question': 'question', 'answer': 'answer', 'evidence_text': 'evidence_text', 'syntheses': 'syntheses', 'Generated_BY_GEMMA': 'Generated_BY_GEMMA'})\n",
        "Few_Shot_Gemma_7.to_csv('Few_Shot_Gemma_7.csv', index=False)\n",
        "FileLink('Few_Shot_Gemma_7.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "XmOUm6EX0-ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_1 = \"/kaggle/input/few-shot-gemma/Few_Shot_Gemma_1.csv\"\n",
        "file_path_2 = \"/kaggle/input/few-shot-gemma/Few_Shot_Gemma_2.csv\"\n",
        "file_path_3 = \"/kaggle/input/few-shot-gemma/Few_Shot_Gemma_3.csv\"\n",
        "file_path_4 = \"/kaggle/input/few-shot-gemma/Few_Shot_Gemma_4.csv\"\n",
        "file_path_5 = \"/kaggle/input/few-shot-gemma/Few_Shot_Gemma_5.csv\"\n",
        "file_path_6 = \"/kaggle/input/few-shot-gemma/Few_Shot_Gemma_6.csv\"\n",
        "file_path_7 = \"/kaggle/input/few-shot-gemma/Few_Shot_Gemma_7.csv\"\n",
        "Few_Shot_Gemma_1 = pd.read_csv(file_path_1)\n",
        "Few_Shot_Gemma_2 = pd.read_csv(file_path_2)\n",
        "Few_Shot_Gemma_3 = pd.read_csv(file_path_3)\n",
        "Few_Shot_Gemma_4 = pd.read_csv(file_path_4)\n",
        "Few_Shot_Gemma_5 = pd.read_csv(file_path_5)\n",
        "Few_Shot_Gemma_6 = pd.read_csv(file_path_6)\n",
        "Few_Shot_Gemma_7 = pd.read_csv(file_path_7)"
      ],
      "metadata": {
        "trusted": true,
        "id": "BZkWw9LR0-ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Few_Shot_Gemma_Merged = pd.concat([Few_Shot_Gemma_1, Few_Shot_Gemma_2, Few_Shot_Gemma_3, Few_Shot_Gemma_4, Few_Shot_Gemma_5, Few_Shot_Gemma_6, Few_Shot_Gemma_7], ignore_index=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "q6yk8POg0-ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Selecting the 'answer' and 'Generated_BY_GEMMA' columns\n",
        "# selected_columns = ['answer', 'Generated_BY_GEMMA']\n",
        "\n",
        "# # Printing the first 50 rows with a dash line between the columns\n",
        "# for index, row in Few_Shot_Gemma_Merged[selected_columns].head(50).iterrows():\n",
        "#     print(f\"{row['answer']} --------- {row['Generated_BY_GEMMA']}\")\n",
        "#     print('--' * 50)  # Dash line separator"
      ],
      "metadata": {
        "trusted": true,
        "id": "yci0xSwE0-ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "Few_Shot_Gemma_Merged.to_csv('Few_Shot_Gemma_Merged.csv', index=False)\n",
        "FileLink('Few_Shot_Gemma_Merged.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "cmSi7dE_0-ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Few_Shot_Gemma_Merged)"
      ],
      "metadata": {
        "trusted": true,
        "id": "U7rlyFDG0-ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Few Shot"
      ],
      "metadata": {
        "id": "ZoUCCTct0-Za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from typing import List\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "trusted": true,
        "id": "N1kuPKeb0-Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rouge_l(candidate, reference):\n",
        "    m, n = len(candidate), len(reference)\n",
        "    #print(m,n)\n",
        "    dp_table = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if candidate[i - 1] == reference[j - 1]:\n",
        "                dp_table[i][j] = dp_table[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                dp_table[i][j] = max(dp_table[i - 1][j], dp_table[i][j - 1])\n",
        "\n",
        "    return dp_table[m][n] / n"
      ],
      "metadata": {
        "trusted": true,
        "id": "RIGW-xbz0-Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "trusted": true,
        "id": "Yn6fLKEN0-Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "trusted": true,
        "id": "dseB31w40-Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity_score(answer,syntheses):\n",
        "    tokens_answer = preprocess_text(answer)\n",
        "    tokens_syntheses = preprocess_text(syntheses)\n",
        "    str_answer = ' '.join(tokens_answer)\n",
        "    str_syntheses = ' '.join(tokens_syntheses)\n",
        "    freqdist_answer = nltk.FreqDist(str_answer.split())\n",
        "    freqdist_syntheses = nltk.FreqDist(str_syntheses.split())\n",
        "    # Extract frequencies for unique tokens in both texts\n",
        "    unique_tokens = set(freqdist_answer.keys()).union(freqdist_syntheses.keys())\n",
        "\n",
        "    freq_answer = [freqdist_answer[token] for token in unique_tokens]\n",
        "    freq_syntheses = [freqdist_syntheses[token] for token in unique_tokens]\n",
        "\n",
        "    vector_answer = np.array(freq_answer).reshape(1, -1)\n",
        "    vector_syntheses = np.array(freq_syntheses).reshape(1, -1)\n",
        "\n",
        "    similarity_score = cosine_similarity(vector_answer, vector_syntheses)[0][0]\n",
        "\n",
        "    return similarity_score"
      ],
      "metadata": {
        "trusted": true,
        "id": "ro_9kGbW0-Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer\n",
        "\n",
        "total_average_rouge_l_scores_1 = []\n",
        "total_average_cosine_similarity_scores_1  = []\n",
        "\n",
        "num_labels_1 = []\n",
        "count = 0\n",
        "\n",
        "for i in range(5):\n",
        "    count += 10\n",
        "    num_labels_1.append(count)\n",
        "    #print(num_labels)\n",
        "    df = Few_Shot_Gemma_Merged.head(count)\n",
        "    #print(len(df))\n",
        "\n",
        "    rouge_l_scores = []\n",
        "    cosine_similarity_scores = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        answer = row['answer']\n",
        "        syntheses = row['Generated_BY_GEMMA']\n",
        "\n",
        "        compute_sim_score = compute_similarity_score(answer,syntheses)\n",
        "        cosine_similarity_scores.append(compute_sim_score)\n",
        "\n",
        "        rouge_l_score = compute_rouge_l(answer, syntheses)\n",
        "        rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "\n",
        "    total_average_cosine_similarity_score = sum(cosine_similarity_scores) / len(cosine_similarity_scores)\n",
        "    total_average_cosine_similarity_scores_1.append(total_average_cosine_similarity_score)\n",
        "\n",
        "\n",
        "    total_average_rouge_l_score = sum(rouge_l_scores)/len(rouge_l_scores)\n",
        "    total_average_rouge_l_scores_1.append(total_average_rouge_l_score)\n",
        "\n",
        "    print(total_average_cosine_similarity_scores_1)\n",
        "    print(total_average_rouge_l_scores_1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "KYVINA8z0-Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(num_labels_1, total_average_rouge_l_scores_1, marker='o', linestyle='-', color='skyblue', linewidth=2)\n",
        "\n",
        "plt.xlabel('Number of Labels')\n",
        "plt.ylabel('Total Average ROUGE-L Score for 5 Epoch')\n",
        "plt.title(' After Fine-Tuning ROUGE-L Score Comparison for 5 Epoch')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(num_labels_1, total_average_cosine_similarity_scores_1, marker='o', linestyle='-', color='skyblue', linewidth=2)\n",
        "plt.xlabel('Number of Labels')\n",
        "plt.ylabel('Total Average Cosine Similarity Score for 5 Epoch')\n",
        "plt.title(' After Fine-Tuning Cosine Similarity Score for 5 Epoch')"
      ],
      "metadata": {
        "trusted": true,
        "id": "VnWnwnic0-Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shot Finished"
      ],
      "metadata": {
        "id": "avFLBDzl0-Zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning Preprocessing"
      ],
      "metadata": {
        "id": "6-sBLyEd0-Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Finance_Reduced_Train.drop('syntheses', axis=1, inplace=True)\n",
        "Finance_Reduced_Train.columns"
      ],
      "metadata": {
        "trusted": true,
        "id": "8HeKvJ4h0-Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Finance_Reduced_Train_Reduced_80 = Finance_Reduced_Train.head(80)\n",
        "Validation_20 = Finance_Reduced_Train.tail(20)\n",
        "len(Finance_Reduced_Train_Reduced_80)\n",
        "len(Validation_20)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ROs9zXIK0-Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "prompts = []\n",
        "for idx, row in Finance_Reduced_Train_Reduced_80.iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    evidence_text = row['evidence_text']  # Assuming you have access to this column\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"You are a financial chatbot trained to answer questions based on the information provided.\n",
        "Your responses should be directly sourced from the content of these evidence_text(context).\n",
        "When asked a question, ensure that your answer is explicitly supported by the text and do not\n",
        "include any external information, interpretations, or assumptions not clearly stated in the evidence_text(context).\n",
        "If a question pertains to financial data or analysis that is not explicitly covered in the evidence_text(context) provided,\n",
        "respond by stating that the information is not available in the evidence_text(context).\n",
        "Your primary focus should be on accuracy, specificity, and adherence to the information in the evidence_text(context),\n",
        "particularly regarding financial statements, company performance, and market positions.\"\"\"\n",
        "\n",
        "    prompt = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n{question}\\n{evidence_text}\\n<end_of_turn>\\n<start_of_turn>model\\n{answer}\\n<end_of_turn>\"\n",
        "    prompts.append(prompt)\n",
        "\n",
        "dataset = Dataset.from_pandas(pd.DataFrame({'text': prompts}))\n",
        "dataset\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "m3-fsMje0-Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = []\n",
        "for idx, row in Validation_20.iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    evidence_text = row['evidence_text']  # Assuming you have access to this column\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"You are a financial chatbot trained to answer questions based on the information provided.\n",
        "Your responses should be directly sourced from the content of these evidence_text(context).\n",
        "When asked a question, ensure that your answer is explicitly supported by the text and do not\n",
        "include any external information, interpretations, or assumptions not clearly stated in the evidence_text(context).\n",
        "If a question pertains to financial data or analysis that is not explicitly covered in the evidence_text(context) provided,\n",
        "respond by stating that the information is not available in the evidence_text(context).\n",
        "Your primary focus should be on accuracy, specificity, and adherence to the information in the evidence_text(context),\n",
        "particularly regarding financial statements, company performance, and market positions.\"\"\"\n",
        "\n",
        "    prompt = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n{question}\\n{evidence_text}\\n<end_of_turn>\\n<start_of_turn>model\\n{answer}\\n<end_of_turn>\"\n",
        "    prompts.append(prompt)\n",
        "\n",
        "eval_dataset = Dataset.from_pandas(pd.DataFrame({'text': prompts}))\n",
        "eval_dataset\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "u78yop950-Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "jvbWMybv0-Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "id": "zqtgEBsu0-Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ef3R3BG10-Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=64,\n",
        "    # target_modules=[\"query_key_value\"],\n",
        "    target_modules=['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj'], #specific to Gemma models.\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "SRDVzUri0-Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Qwm7Hk_P0-Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For 5 Epoches"
      ],
      "metadata": {
        "id": "lXGws0ih0-Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    \"Gemma_5_Epoch\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=0.3,\n",
        "    num_train_epochs=5,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    warmup_ratio=0.05,\n",
        "    save_strategy=\"epoch\",\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=42,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "trusted": true,
        "id": "zEbxpSGC0-Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=100,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "Fine_tuned_5_Epoch = trainer.train()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Lk7PrxBN0-Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For 10 Epoches"
      ],
      "metadata": {
        "id": "HS0boYGm0-Zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    \"Gemma_Finance_80_10_Epoch\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=0.3,\n",
        "    num_train_epochs=5,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    warmup_ratio=0.05,\n",
        "    save_strategy=\"epoch\",\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=42,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "trusted": true,
        "id": "WWxiFPoB0-Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=100,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "Fine_tuned_80_10_Epoch = trainer.train()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "r3griEOP0-Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For 15 Epoches"
      ],
      "metadata": {
        "id": "y25Zfsvj0-Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    \"Gemma_Finance_80_15_Epoch_2nd\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=0.3,\n",
        "    num_train_epochs=5,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    warmup_ratio=0.05,\n",
        "    save_strategy=\"epoch\",\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=42,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "trusted": true,
        "id": "q6tUUp0o0-Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=100,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "Fine_tuned_80_15_Epoch = trainer.train()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "WnJx8dtN0-Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "iI4vREBn0-Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model"
      ],
      "metadata": {
        "trusted": true,
        "id": "zfMKfUFo0-Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import transformers\n",
        "\n",
        "model_id_1 = '/kaggle/working/Gemma_Finance_80_5_Epoch'\n",
        "model_id_2 = '/kaggle/working/Gemma_Finance_80_10_Epoch'\n",
        "model_id_3 = '/kaggle/working/Gemma_Finance_80_15_Epoch_2nd'\n",
        "\n",
        "#device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# fine_tuned_gemma_5_Epoch = AutoModelForCausalLM.from_pretrained(model_id_1, quantization_config=bnb_config, device_map=\"auto\")\n",
        "# tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1)\n",
        "# tokenizer_1.pad_token = tokenizer_1.eos_token\n",
        "# tokenizer_1.padding_side = \"right\"\n",
        "\n",
        "# fine_tuned_gemma_10_Epoch = AutoModelForCausalLM.from_pretrained(model_id_2, quantization_config=bnb_config, device_map=\"auto\")\n",
        "# tokenizer_2 = AutoTokenizer.from_pretrained(model_id_2)\n",
        "# tokenizer_2.pad_token = tokenizer_2.eos_token\n",
        "# tokenizer_2.padding_side = \"right\"\n",
        "\n",
        "fine_tuned_gemma_15_Epoch = AutoModelForCausalLM.from_pretrained(model_id_3, quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer_3 = AutoTokenizer.from_pretrained(model_id_3)\n",
        "tokenizer_3.pad_token = tokenizer_3.eos_token\n",
        "tokenizer_3.padding_side = \"right\""
      ],
      "metadata": {
        "trusted": true,
        "id": "xh4kepRH0-Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path_2 = \"/kaggle/input/finance-bench-dataset/syntheses_10.csv\"\n",
        "Finance_data = pd.read_csv(file_path_2)\n",
        "print(Finance_data.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "QTl618Z80-Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Finance_data.drop('syntheses', axis=1, inplace=True)\n",
        "Finance_data.columns"
      ],
      "metadata": {
        "trusted": true,
        "id": "GMKKOhOT0-Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Finance_Reduced_Test= Finance_data.head(50)\n",
        "len(Finance_Reduced_Test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "6Vb1Cr6z0-Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"Give answer to questions provided below from the evidence text.\"\"\"\n",
        "\n",
        "count = 0\n",
        "syntheses_with_gemma = []\n",
        "\n",
        "for idx, row in Finance_Reduced_Test.iterrows():\n",
        "    question = row['question']\n",
        "    evidence_text = row['evidence_text']\n",
        "\n",
        "    prompt = f\"\"\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\n",
        "        {question}\\n{evidence_text}\\n<end_of_turn>\\n\"\"\"\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer_3(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate response\n",
        "    output = fine_tuned_gemma_15_Epoch.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=100)\n",
        "\n",
        "    # Decode the response\n",
        "    generated_text = tokenizer_3.decode(output[0], skip_special_tokens=True)\n",
        "    #print(generated_text)\n",
        "\n",
        "    #Finding Extracted Answer\n",
        "    index_of_answer = generated_text.find(\"<end_of_turn>\")\n",
        "\n",
        "    # Extract the text after \"Answer:\"\n",
        "    answer_text = generated_text[index_of_answer + len(\"<end_of_turn>\"):].strip()\n",
        "    syntheses_with_gemma.append(answer_text)\n",
        "    #print(answer_text)\n",
        "\n",
        "    #Comparing the answer with the base answer\n",
        "    dash_line = '-'.join('' for x in range(100))\n",
        "    Answer = Finance_Reduced_Test.loc[idx, 'answer']\n",
        "#     print(f'BASELINE HUMAN ANSWER:\\n{Answer}\\n')\n",
        "#     print(dash_line)\n",
        "#     print(f'MODEL GENERATION - ZERO SHOT:\\n{answer_text}')\n",
        "#     print(dash_line)\n",
        "    print(count+1)\n",
        "    count = count + 1\n",
        "\n",
        "#Appending it to main file\n",
        "Finance_Reduced_Test['Generated_BY_GEMMA'] = syntheses_with_gemma"
      ],
      "metadata": {
        "trusted": true,
        "id": "9hkgFTdz0-Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "fine_tuned_15_Epoch = Finance_Reduced_Test.rename(columns={'question': 'question', 'answer': 'answer', 'evidence_text': 'evidence_text', 'Generated_BY_Gemma': 'Generated_BY_Gemma'})\n",
        "fine_tuned_15_Epoch.to_csv('fine_tuned_15_Epoch_Gemma.csv', index=False)\n",
        "FileLink('fine_tuned_15_Epoch_Gemma.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q9g48pcq0-Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_5_Epoch.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "NmeIkU200-Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_5_Epoch['Generated_BY_Gemma'][17]"
      ],
      "metadata": {
        "trusted": true,
        "id": "3QVe0uk80-Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "EczVggwx0-Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "file_path_1 = \"/kaggle/input/fine-tuned-gemma/fine_tuned_5_Epoch_Gemma (1).csv\"\n",
        "file_path_2 = \"/kaggle/input/fine-tuned-gemma/fine_tuned_10_Epoch_Gemma.csv\"\n",
        "file_path_3 = \"/kaggle/input/fine-tuned-gemma/fine_tuned_15_Epoch_Gemma.csv\"\n",
        "\n",
        "Fine_Tuned_Gemma_5_Epoch = pd.read_csv(file_path_1)\n",
        "Fine_Tuned_Gemma_10_Epoch = pd.read_csv(file_path_2)\n",
        "Fine_Tuned_Gemma_15_Epoch = pd.read_csv(file_path_3)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-06T17:56:28.905246Z",
          "iopub.execute_input": "2024-05-06T17:56:28.905921Z",
          "iopub.status.idle": "2024-05-06T17:56:29.292297Z",
          "shell.execute_reply.started": "2024-05-06T17:56:28.905888Z",
          "shell.execute_reply": "2024-05-06T17:56:29.291525Z"
        },
        "trusted": true,
        "id": "I7q5-SHm0-Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the 'answer' and 'Generated_BY_GEMMA' columns\n",
        "selected_columns = ['answer', 'Generated_BY_GEMMA']\n",
        "\n",
        "# Printing the first 50 rows with a dash line between the columns\n",
        "for index, row in Fine_Tuned_Gemma_5_Epoch[selected_columns].head(50).iterrows():\n",
        "    print(f\"{row['answer']} --------- {row['Generated_BY_GEMMA']}\")\n",
        "    print('--' * 50)  # Dash line separator"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-06T17:56:46.134241Z",
          "iopub.execute_input": "2024-05-06T17:56:46.134627Z",
          "iopub.status.idle": "2024-05-06T17:56:46.151412Z",
          "shell.execute_reply.started": "2024-05-06T17:56:46.134595Z",
          "shell.execute_reply": "2024-05-06T17:56:46.150500Z"
        },
        "trusted": true,
        "id": "FeDJdMm40-Zl",
        "outputId": "cde81b81-1387-46c5-deb9-f67f0bcbf138"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "$1577.00 --------- The text does not specify the FY2018 capital expenditure amount for 3M, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n$8.70 --------- The text does not provide information about the year end FY2018 net PPNE for 3M, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\nNo, the company is managing its CAPEX and Fixed Assets pretty efficiently, which is evident from below key metrics:\nCAPEX/Revenue Ratio: 5.1%\nFixed assets/Total Assets: 20%\nReturn on Assets= 12.4% --------- The text does not explicitly state whether 3M is a capital-intensive business based on FY2022 data, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\nOperating Margin for 3M in FY2022 has decreased by 1.7% primarily due to: \n-Decrease in gross Margin\n-mostly one-off charges including Combat Arms Earplugs litigation, impairment related to exiting PFAS manufacturing, costs related to exiting Russia and divestiture-related restructuring\ncharges --------- **Questions:**\n\n1. What drove the change in operating margin for 3M in FY2022?\n2. Why might operating margin not be a useful metric for a company like 3M?\n\n**Answer:**\n\n**1.** SG&A, measured as a percent of sales, increased in 2022 when compared to the same period last year due to increased special item costs for significant litigation primarily related to steps toward resolving Combat Arms Earplugs litigation, certain\n----------------------------------------------------------------------------------------------------\nThe consumer segment shrunk by 0.9% organically. --------- The text evidence suggests that the segment that has dragged down 3M's overall growth in 2022 is Safety and Industrial.\n----------------------------------------------------------------------------------------------------\nNo. The quick ratio for 3M was 0.96 by Jun'23 close, which needs a bit of an improvement to touch the 1x mark --------- The text does not describe the quick ratio or its relevance to liquidity, therefore I cannot answer the question.\n----------------------------------------------------------------------------------------------------\nFollowing debt securities registered under 3M's name are listed to trade on the New York Stock Exchange:\n-1.500% Notes due 2026 (Trading Symbol: MMM26)\n-1.750% Notes due 2030 (Trading Symbol: MMM30)\n-1.500% Notes due 2031 (Trading Symbol: MMM31) --------- The text describes 3M's debt securities registered to trade on a national securities exchange under their name as of Q2 of 2023. The text lists the debt securities and their corresponding trading symbols and exchanges.\n\n**Answer:**\n\nThe debt securities registered to trade on a national securities exchange under 3M's name as of Q2 of 2023 are:\n\n- 1.500% Notes due 2026 (MMM2\n----------------------------------------------------------------------------------------------------\nYes, not only they distribute the dividends on a routine basis, 3M has also been increasing the per share dividend for consecutive 65 years --------- The text suggests that 3M has been consistently increasing its dividends for the past 65 years. Therefore, the answer to the question is yes.\n----------------------------------------------------------------------------------------------------\n24.26 --------- The text does not provide information about the FY2019 fixed asset turnover ratio for Activision Blizzard, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n1.9% --------- The text does not provide information about the FY2017 - FY2019 3 year average of capex as a % of revenue for Activision Blizzard, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n0.66 --------- The text provided does not specify the question or answer related to the text therefore I am unable to provide the answer to the question.\n----------------------------------------------------------------------------------------------------\n65.4% --------- The text does not explicitly state the year-over-year change in unadjusted operating income from FY2015 to FY2016, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n0.83 --------- The text provided describes Adobe Systems Incorporated's consolidated financial statements for the year ended December 1, 2017. The text includes the company's consolidated balance sheet, consolidated statement of cash flows, and supplemental disclosures.\n\nPlease answer the following questions based on the text:\n\n1. What is the FY2017 operating cash flow ratio for Adobe?\n2. What is the total current liabilities for Adobe at December 1, 2017?\n3\n----------------------------------------------------------------------------------------------------\nNo the operating margins of Adobe have recently declined from 36.8% in FY 2021 to 34.6% in FY2022. A drop by 2.2% in a year. --------- **Questions:**\n\n1. Does Adobe have an improving operating margin profile as of FY2022? Explain your answer based on the evidence text.\n2. What is the primary source of revenue for Adobe as of FY2022?\n\n**Answer:**\n\n**1.** The text does not provide information about Adobe's operating margin profile as of FY2022, therefore I cannot answer this question.\n\n**2.** The text states that the primary source of revenue\n----------------------------------------------------------------------------------------------------\nYes, the FCF conversion (using net income as the denominator) for Adobe has improved by ~13% from 143% in 2021 to 156% in 2022 --------- Sure, here is the answer to the question:\n\nThe text does not provide evidence to suggest whether Adobe has an improving Free cashflow conversion as of FY2022, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n0 --------- The text does not explicitly state the quantity of restructuring costs directly outlined in AES Corporation's income statements for FY2022, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\nAES has converted inventory 9.5 times in FY 2022. --------- **Questions:**\n\n1. Calculate the inventory turnover ratio for the FY2022.\n2. Explain why conventional inventory management is not meaningful for the company.\n\n**Answer:**\n\n**1.** The text does not provide information about the number of times AES Corporation sold its inventory in FY2022, therefore I cannot answer this question.\n\n**2.** The text does not explain why conventional inventory management is not meaningful for the company, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n-0.02 --------- The text does not provide information about AES's FY2022 return on assets (ROA), therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n93.86 --------- The text does not provide information about Amazon's FY2017 days payable outstanding (DPO), therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n30.8% --------- The text does not explicitly state the year-over-year change in revenue from FY2016 to FY2017 for Amazon, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n$11588.00 --------- The text does not provide information about Amazon's FY2019 net income attributable to shareholders, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n$1616.00 --------- The text does not explicitly state Amcor's year end FY2020 net AR (in USD millions) therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\nAmcor Finance (USA), Inc. and Amcor Flexibles North America, Inc., entered into supplemental indentures relating to Guaranteed Senior Notes due 2026 and 2028. This involved the substitution of the Substitute Issuer (Amcor Flexibles North America) for the Former Issuer (Amcor Finance) and the assumption of covenants under the indentures. (In essence a novation agreement) --------- The text does not explicitly state the key agenda of the AMCOR's 8k filing dated 1st July 2022, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\nThe quick ratio has slightly improved from 0.67 times to 0.69 times between FY 2023 and FY 2022.(3.4% jump) --------- Sure, here is the answer to the question:\n\nThe text does not provide information about AMCOR's quick ratio, therefore I cannot answer the question.\n----------------------------------------------------------------------------------------------------\nAmcor completed these acquisitions during FY2023:\n-100% equity interest of a flexibles manufacturing company in the Czech Republic\n- 100% equity interest in a medical device packaging manufacturing site in\nShanghai, China.\n-acquisition of a New Zealand-based leading manufacturer of state-of-the-art, automated protein\npackaging machines. --------- **Questions:**\n\n1. What major acquisitions did AMCOR complete in FY2023, FY2022 and FY2021?\n2. What is the significance of the acquisitions completed by AMCOR in FY2023, FY2022 and FY2021?\n3. What is the impact of the acquisitions completed by AMCOR in FY2023, FY2022 and FY2021 on the Company\n----------------------------------------------------------------------------------------------------\nAmcor is a global leader in packaging production for various use cases. --------- The text states that AMCOR primarily operates in the packaging industry.\n----------------------------------------------------------------------------------------------------\nNo. For AMCOR there has been a slight decline in gross margins by 0.8%. --------- ## Answer\n\nThe text does not provide evidence to suggest that AMCOR has an improving gross margin profile as of FY2023. Therefore, I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n87% of the total restructuring liability is related Employee liabilities. --------- **Questions:**\n\n1. What is the total restructuring costs incurred by AMCOR in Q2 of FY2023?\n2. What was the liability balance at December 31, 2022 for AMCOR's restructuring liability?\n\n**Answer:**\n\n**1.** The total restructuring costs incurred by AMCOR in Q2 of FY2023 are $25 million.\n\n**2.** The liability balance at December 31, 20\n----------------------------------------------------------------------------------------------------\nAMCOR's Adj. EBITDA was $2,018mn in FY 2023 --------- The text does not provide information about AMCOR's Adjusted Non GAAP EBITDA for FY 2023, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\nThe Real Growth was flat in FY 2023 vs FY 2022. --------- The text does not specify the Real change in Sales for AMCOR in FY 2023 vs FY 2022, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n4.2% --------- The text does not provide information about the FY2015 depreciation and amortization (D&A from cash flow statement) % margin for AMD, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\nYes. The quick ratio is 1.57, calculated as (cash and cash equivalents+Short term investments+Accounts receivable, net+receivables from related parties)/ (current liabilities). --------- The text does not describe the quick ratio or its relevance to measure liquidity, therefore I cannot answer the question.\n----------------------------------------------------------------------------------------------------\nAMD sells server microprocessors (CPUs) and graphics processing units (GPUs), data processing units (DPUs), Field Programmable Gate Arrays (FPGAs), and Adaptive System-on-Chip (SoC) products for data centers; CPUs, accelerated processing units (APUs) that integrate CPUs and GPUs, and chipsets for desktop and notebook personal computers; discrete GPUs, and semi-custom SoC products and development services; and embedded CPUs, GPUs, APUs, FPGAs, and Adaptive SoC products. --------- The text describes AMD's major products and services as of FY22. What are the major products and services that AMD sells as of FY22?\n----------------------------------------------------------------------------------------------------\nIn 2022, AMD reported Higher sales of their EPYC server processors, higher semi-custom product sales, and the inclusion of Xilinx embedded product sales --------- **Questions:**\n\n1. What was the net revenue for 2022 for AMD?\n2. What drove the increase in net revenue for 2022 compared to 2021?\n\n**Answer:**\n\n**1.** $23.6 billion\n\n**2.** The increase in net revenue for 2022 was driven by a 64% increase in Data Center segment revenue primarily due to higher sales of our EPYC™ server processors,\n----------------------------------------------------------------------------------------------------\nThe decrease in AMD's operating income was primarily driven by amortization of intangible assets associated with the Xilinx acquisition --------- **Questions:**\n\n1. What was the primary driver of the decrease in operating income for 2022 compared to 2021?\n2. Is operating margin a useful metric for a company like AMD? Explain your answer.\n\n**Answer:**\n\n**1.** The primary driver of the decrease in operating income for 2022 compared to 2021 was the amortization of intangible assets associated with the Xilinx acquisition.\n\n**2.** Operating margin\n----------------------------------------------------------------------------------------------------\nIn 2022, AMD brought in the most cashflow from Operations --------- The text describes the cash flow activities of Advanced Micro Devices, Inc. in FY22.\n\n**Questions:**\n\n1. Which of the operating, investing, or financing activities brought in the most cash flow for AMD in FY22?\n2. What was the net cash provided by operating activities for AMD in FY22?\n3. What was the net cash used in investing activities for AMD in FY22?\n4. What was the net cash used in financing activities for\n----------------------------------------------------------------------------------------------------\nData Center --------- **Questions:**\n\n1. Which AMD reporting segment did sales proportionally increase the most from FY21 to FY22, excluding Embedded?\n2. What was the total net revenue for the year ended December 31, 2022?\n\n**Answer:**\n\n**1.** Client sales proportionally increased the most from FY21 to FY22, excluding Embedded.\n\n**2.** The total net revenue for the year ended December 31, 2022\n----------------------------------------------------------------------------------------------------\nYes, one customer accounted for 16% of consolidated net revenue --------- **Questions:**\n\n1. What percentage of consolidated net revenue did one customer account for in FY22?\n2. What segment of products did sales to this customer consist of?\n3. What would be the effect of losing this customer on the business?\n\n**Answer:**\n\n1. 16%\n2. Gaming\n3. Material adverse effect\n----------------------------------------------------------------------------------------------------\nThere are none --------- The text describes American Express' debt securities registered to trade on a national securities exchange. It states that as of 2022, Common Shares are the only debt securities registered to trade on a national securities exchange under American Express' name.\n\n**Therefore, the answer to the question is:**\n\nCommon Shares (par value $0.20 per Share)\n----------------------------------------------------------------------------------------------------\nUnited States, EMEA, APAC, and LACC --------- The text does not explicitly state the geographies that American Express primarily operates in as of 2022, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\nPerformance is not measured through operating margin --------- The text does not provide information about AMEX's operating margin profile as of 2022, therefore I cannot answer the question.\n----------------------------------------------------------------------------------------------------\nPerformance is not measured through gross margin --------- The text does not explain the reason for the gross margin change as of the FY2022 for American Express, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\nThe effective tax rate for American Express has changed/dropped from 24.6% in FY 2021 to 21.6% in FY 2022. --------- The text states that the effective tax rate of American Express has increased from 21.6% in FY2021 to 24.6% in FY2022.\n----------------------------------------------------------------------------------------------------\nCustomer deposits --------- The text does not specify the largest liability in American Express's Balance Sheet in 2022, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\nYes --------- The text suggests that American Express was able to retain card members during 2022.\n----------------------------------------------------------------------------------------------------\n$0.40 --------- The text does not specify the amount of cash dividends paid out by American Water Works in FY2020, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n$1832.00 --------- The text does not specify the question therefore I am unable to answer the question.\n----------------------------------------------------------------------------------------------------\nYes. American Water Works had postivie working capital of $ 124Mn by FY 2022. --------- The text does not explicitly state whether American Water Works has positive working capital based on FY2022 data. Therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n2.8% --------- The text does not provide information about the FY2015 - FY2017 3 year average net profit margin (as a %) for Best Buy, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n$5409.00 --------- The text does not provide information about the year end FY2019 total amount of inventories for Best Buy, therefore I cannot answer this question.\n----------------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from typing import List\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "trusted": true,
        "id": "mYgKH7nX0-Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rouge_l(candidate, reference):\n",
        "    m, n = len(candidate), len(reference)\n",
        "    #print(m,n)\n",
        "    dp_table = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if candidate[i - 1] == reference[j - 1]:\n",
        "                dp_table[i][j] = dp_table[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                dp_table[i][j] = max(dp_table[i - 1][j], dp_table[i][j - 1])\n",
        "\n",
        "    return dp_table[m][n] / n"
      ],
      "metadata": {
        "trusted": true,
        "id": "tv10lIlk0-Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "trusted": true,
        "id": "QxEJtB_S0-Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "trusted": true,
        "id": "2XXf_9zj0-Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity_score(answer,syntheses):\n",
        "    tokens_answer = preprocess_text(answer)\n",
        "    tokens_syntheses = preprocess_text(syntheses)\n",
        "    str_answer = ' '.join(tokens_answer)\n",
        "    str_syntheses = ' '.join(tokens_syntheses)\n",
        "    freqdist_answer = nltk.FreqDist(str_answer.split())\n",
        "    freqdist_syntheses = nltk.FreqDist(str_syntheses.split())\n",
        "    # Extract frequencies for unique tokens in both texts\n",
        "    unique_tokens = set(freqdist_answer.keys()).union(freqdist_syntheses.keys())\n",
        "\n",
        "    freq_answer = [freqdist_answer[token] for token in unique_tokens]\n",
        "    freq_syntheses = [freqdist_syntheses[token] for token in unique_tokens]\n",
        "\n",
        "    vector_answer = np.array(freq_answer).reshape(1, -1)\n",
        "    vector_syntheses = np.array(freq_syntheses).reshape(1, -1)\n",
        "\n",
        "    similarity_score = cosine_similarity(vector_answer, vector_syntheses)[0][0]\n",
        "\n",
        "    return similarity_score"
      ],
      "metadata": {
        "trusted": true,
        "id": "LxQwwKkG0-Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5 Epoch Test"
      ],
      "metadata": {
        "id": "2XAOYXyL0-Zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer\n",
        "\n",
        "total_average_rouge_l_scores_1 = []\n",
        "total_average_cosine_similarity_scores_1  = []\n",
        "\n",
        "num_labels_1 = []\n",
        "count = 0\n",
        "\n",
        "for i in range(5):\n",
        "    count += 10\n",
        "    num_labels_1.append(count)\n",
        "    #print(num_labels)\n",
        "    df = Fine_Tuned_Gemma_5_Epoch.head(count)\n",
        "    #print(len(df))\n",
        "\n",
        "    rouge_l_scores = []\n",
        "    cosine_similarity_scores = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        answer = row['answer']\n",
        "        syntheses = row['Generated_BY_GEMMA']\n",
        "\n",
        "        compute_sim_score = compute_similarity_score(answer,syntheses)\n",
        "        cosine_similarity_scores.append(compute_sim_score)\n",
        "\n",
        "        rouge_l_score = compute_rouge_l(answer, syntheses)\n",
        "        rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "\n",
        "    total_average_cosine_similarity_score = sum(cosine_similarity_scores) / len(cosine_similarity_scores)\n",
        "    total_average_cosine_similarity_scores_1.append(total_average_cosine_similarity_score)\n",
        "\n",
        "\n",
        "    total_average_rouge_l_score = sum(rouge_l_scores)/len(rouge_l_scores)\n",
        "    total_average_rouge_l_scores_1.append(total_average_rouge_l_score)\n",
        "\n",
        "    print(total_average_cosine_similarity_scores_1)\n",
        "    print(total_average_rouge_l_scores_1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "zwp7QYuT0-Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(num_labels_1, total_average_rouge_l_scores_1, marker='o', linestyle='-', color='skyblue', linewidth=2)\n",
        "\n",
        "plt.xlabel('Number of Labels')\n",
        "plt.ylabel('Total Average ROUGE-L Score for 5 Epoch')\n",
        "plt.title(' After Fine-Tuning ROUGE-L Score Comparison for 5 Epoch')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(num_labels_1, total_average_cosine_similarity_scores_1, marker='o', linestyle='-', color='skyblue', linewidth=2)\n",
        "plt.xlabel('Number of Labels')\n",
        "plt.ylabel('Total Average Cosine Similarity Score for 5 Epoch')\n",
        "plt.title(' After Fine-Tuning Cosine Similarity Score for 5 Epoch')"
      ],
      "metadata": {
        "trusted": true,
        "id": "eDyFJW_w0-Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10 Epoch Test"
      ],
      "metadata": {
        "id": "OBWpeSK90-Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer\n",
        "\n",
        "total_average_rouge_l_scores_2 = []\n",
        "total_average_cosine_similarity_scores_2  = []\n",
        "\n",
        "num_labels_2 = []\n",
        "count = 0\n",
        "\n",
        "for i in range(5):\n",
        "    count += 10\n",
        "    num_labels_2.append(count)\n",
        "    #print(num_labels)\n",
        "    df = Fine_Tuned_Gemma_10_Epoch.head(count)\n",
        "    #print(len(df))\n",
        "\n",
        "    rouge_l_scores = []\n",
        "    cosine_similarity_scores = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        answer = row['answer']\n",
        "        syntheses = row['Generated_BY_GEMMA']\n",
        "\n",
        "        compute_sim_score = compute_similarity_score(answer,syntheses)\n",
        "        cosine_similarity_scores.append(compute_sim_score)\n",
        "\n",
        "        rouge_l_score = compute_rouge_l(answer, syntheses)\n",
        "        rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "\n",
        "    total_average_cosine_similarity_score = sum(cosine_similarity_scores) / len(cosine_similarity_scores)\n",
        "    total_average_cosine_similarity_scores_2.append(total_average_cosine_similarity_score)\n",
        "\n",
        "\n",
        "    total_average_rouge_l_score = sum(rouge_l_scores)/len(rouge_l_scores)\n",
        "    total_average_rouge_l_scores_2.append(total_average_rouge_l_score)\n",
        "\n",
        "    #print(total_average_cosine_similarity_scores_1)\n",
        "    #print(total_average_rouge_l_scores_1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "k8Y3MVxP0-Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(num_labels_2, total_average_rouge_l_scores_2, marker='o', linestyle='-', color='skyblue', linewidth=2)\n",
        "\n",
        "plt.xlabel('Number of Labels')\n",
        "plt.ylabel('Total Average ROUGE-L Score for 5 Epoch')\n",
        "plt.title(' After Fine-Tuning ROUGE-L Score Comparison for 5 Epoch')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(num_labels_2, total_average_cosine_similarity_scores_2, marker='o', linestyle='-', color='skyblue', linewidth=2)\n",
        "plt.xlabel('Number of Labels')\n",
        "plt.ylabel('Total Average Cosine Similarity Score for 5 Epoch')\n",
        "plt.title(' After Fine-Tuning Cosine Similarity Score for 5 Epoch')"
      ],
      "metadata": {
        "trusted": true,
        "id": "G6ukpKXV0-Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15 Epoch Test"
      ],
      "metadata": {
        "id": "Un6Ms-VZ0-Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer\n",
        "\n",
        "total_average_rouge_l_scores_3 = []\n",
        "total_average_cosine_similarity_scores_3  = []\n",
        "\n",
        "num_labels_3 = []\n",
        "count = 0\n",
        "\n",
        "for i in range(5):\n",
        "    count += 10\n",
        "    num_labels_3.append(count)\n",
        "    #print(num_labels)\n",
        "    df = Fine_Tuned_Gemma_15_Epoch.head(count)\n",
        "    #print(len(df))\n",
        "\n",
        "    rouge_l_scores = []\n",
        "    cosine_similarity_scores = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        answer = row['answer']\n",
        "        syntheses = row['Generated_BY_GEMMA']\n",
        "\n",
        "        compute_sim_score = compute_similarity_score(answer,syntheses)\n",
        "        cosine_similarity_scores.append(compute_sim_score)\n",
        "\n",
        "        rouge_l_score = compute_rouge_l(answer, syntheses)\n",
        "        rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "\n",
        "    total_average_cosine_similarity_score = sum(cosine_similarity_scores) / len(cosine_similarity_scores)\n",
        "    total_average_cosine_similarity_scores_3.append(total_average_cosine_similarity_score)\n",
        "\n",
        "\n",
        "    total_average_rouge_l_score = sum(rouge_l_scores)/len(rouge_l_scores)\n",
        "    total_average_rouge_l_scores_3.append(total_average_rouge_l_score)\n",
        "\n",
        "    #print(total_average_cosine_similarity_scores_1)\n",
        "    #print(total_average_rouge_l_scores_1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "GiL8RzHT0-Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(num_labels_3, total_average_rouge_l_scores_3, marker='o', linestyle='-', color='skyblue', linewidth=2)\n",
        "\n",
        "plt.xlabel('Number of Labels')\n",
        "plt.ylabel('Total Average ROUGE-L Score for 5 Epoch')\n",
        "plt.title(' After Fine-Tuning ROUGE-L Score Comparison for 5 Epoch')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(num_labels_3, total_average_cosine_similarity_scores_3, marker='o', linestyle='-', color='skyblue', linewidth=2)\n",
        "plt.xlabel('Number of Labels')\n",
        "plt.ylabel('Total Average Cosine Similarity Score for 5 Epoch')\n",
        "plt.title(' After Fine-Tuning Cosine Similarity Score for 5 Epoch')"
      ],
      "metadata": {
        "trusted": true,
        "id": "WaD-B8ss0-Zq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}